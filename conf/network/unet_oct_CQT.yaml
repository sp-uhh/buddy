_target_: "networks.unet_octCQT.Unet_octCQT"

use_norm: True

depth: 7 #total depth of the network (including the first stage)

emb_dim: 256 #dimensionality of the RFF embeddings

Ns: [64, 96 ,96, 128, 128,256, 256] 
#Ns: [8, 8 ,8, 8, 16,16, 16] 

#attention_layers: [0, 0, 0, 0, 1, 1, 1, 1] #num_octs+bottleneck
attention_layers: [0, 0, 0, 0, 0, 0, 0, 0] #num_octs+bottleneck
#attention_Ns: [0, 0, 0, 0,256 ,512,1024 ,1024]

#Ns: [8,8,16,16,32,32,64] 
Ss: [2,2,2, 2, 2, 2, 2] #downsample factors at the first stage, now it is ignored

num_dils: [2,3,4,5,6,7,7]

#I don't like hardcoding these parameters here, as they should go in "exp", but this is the easiest
sample_rate: 22050
audio_len: 184184
cqt:
    window: "kaiser"
    beta: 1
    num_octs: 7
    bins_per_oct: 64 #this needs to be lower than 64, otherwise the time attention is inpractical


#inner_Ns: [64, 64, 64, 64]
#if 4x2, then down factor of 16!

bottleneck_type: "res_dil_convs"

num_bottleneck_layers: 1

attention_dict:
    num_heads: 8
    attn_dropout: 0.0
    bias_qkv: False
    N: 0
    rel_pos_num_buckets: 32
    rel_pos_max_distance: 64
    use_rel_pos: True
    Nproj: 8
